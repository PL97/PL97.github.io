<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    
    <title>Le Peng</title>
    
    
    <description>I am Le peng, a fourth-year Ph.D. student at University of Minnesota (UMN)</description>
    
    <link>http://localhost:4003/</link>
    <atom:link href="http://localhost:4003/feed.xml" rel="self" type="application/rss+xml" />
    
    
      <item>
        <title>Reading</title>
        <description>
          
          NER_eval A simple implementation of strict/lenient matching to evaluate NER performance (precision, recall, F1-score) in 60 lines! This script currently only supports the IOB2 format with both strict and lenient modes. Installation pip install ner_metrics Usage from ner_metrics import classication_report y_true = [['B-PER', 'I-PER', 'O', 'B-ORG', 'B-ORG', 'O', 'O', 'B-PER',...
        </description>
        <pubDate>Sat, 11 Mar 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4003/2023-03-11-ner-metrics/</link>
        <guid isPermaLink="true">http://localhost:4003/2023-03-11-ner-metrics/</guid>
      </item>
    
      <item>
        <title>Reading</title>
        <description>
          
          :fire: Explicit and Implicit Knowledge Distillation via Unlabeled Data :fire: Date: 2023-02-20 :link: #tag/computer_vision #tag/knowledge-distillation Key idea this paper proposed a knowledge distillation method that consider prediction entropy as a method for selecting subsitute dataset from wild (a vein of knowledge distillation method that make use of unlabeld public dataset)....
        </description>
        <pubDate>Mon, 20 Feb 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4003/2023-02-20-knowledge-distillation/</link>
        <guid isPermaLink="true">http://localhost:4003/2023-02-20-knowledge-distillation/</guid>
      </item>
    
      <item>
        <title>Reading</title>
        <description>
          
          :fire: Robust Representation Learning with Self-Distillation for Domain Generalization :fire: Date: 2023-02-19 :link: #tag/computer_vision #tag/self-distillation Key idea utilize self-distillation to learn domain-invariant features to improve domain generalization. It is claimed to be the first few work on ViT key design: feed origional image x to NN to get logit output...
        </description>
        <pubDate>Sun, 19 Feb 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4003/2023-02-19-self.distillation/</link>
        <guid isPermaLink="true">http://localhost:4003/2023-02-19-self.distillation/</guid>
      </item>
    
  </channel>
</rss>
