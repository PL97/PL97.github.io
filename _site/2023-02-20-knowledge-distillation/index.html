<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Explicit and Implicit Knowledge Distillation via Unlabeled Data</title>

  
  <meta name="author" content="Le Peng">
  

  <meta name="description" content=":fire: Explicit and Implicit Knowledge Distillation via Unlabeled Data :fire: Date: 2023-02-20 :link: #tag/computer_vision #tag/knowledge-distillation Key idea this paper proposed a knowledge distillation method that consider prediction entropy as a method for selecting subsitute dataset from wild (a vein of knowledge distillation method that make use of unlabeld public dataset)....">

  

  

  
  <link rel="alternate" type="application/rss+xml" title="Le Peng" href="http://localhost:4003/feed.xml">
  

  

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Le Peng">
  <meta property="og:title" content="Explicit and Implicit Knowledge Distillation via Unlabeled Data">
  <meta property="og:description" content=":fire: Explicit and Implicit Knowledge Distillation via Unlabeled Data :fire: Date: 2023-02-20 :link: #tag/computer_vision #tag/knowledge-distillation Key idea this paper proposed a knowledge distillation method that consider prediction entropy as a method for selecting subsitute dataset from wild (a vein of knowledge distillation method that make use of unlabeld public dataset)....">

  
  <meta property="og:image" content="http://localhost:4003/assets/img/path.jpg">
  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Le Peng">
  <meta property="og:article:published_time" content="2023-02-20T00:00:00-05:00">
  <meta property="og:url" content="http://localhost:4003/2023-02-20-knowledge-distillation/">
  <link rel="canonical" href="http://localhost:4003/2023-02-20-knowledge-distillation/">
  

  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@LePeng57300967">
  <meta name="twitter:creator" content="@LePeng57300967">

  <meta property="twitter:title" content="Explicit and Implicit Knowledge Distillation via Unlabeled Data">
  <meta property="twitter:description" content=":fire: Explicit and Implicit Knowledge Distillation via Unlabeled Data :fire: Date: 2023-02-20 :link: #tag/computer_vision #tag/knowledge-distillation Key idea this paper proposed a knowledge distillation method that consider prediction entropy as a method for selecting subsitute dataset from wild (a vein of knowledge distillation method that make use of unlabeld public dataset)....">

  
  <meta name="twitter:image" content="http://localhost:4003/assets/img/path.jpg">
  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4003/">Le Peng</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/pages/publication">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/pages/services">Service</a>
          </li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Blogs</a>
            <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/pages/blog">Posts</a>
                  <a class="dropdown-item" href="/tags">Tags</a>
            </div>
          </li>
        
          <li class="nav-item">
            <a class="nav-link" href="/index">About Me</a>
          </li></ul>
  </div>

  

  

</nav>





  <!-- TODO this file has become a mess, refactor it -->






  <div id="header-big-imgs" data-num-img=1
    
    
    
      
      data-img-src-1="http://localhost:4003/assets/img/path.jpg"
    
    
    
  ></div>


<header class="header-section has-img">

<div class="big-img intro-header">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Explicit and Implicit Knowledge Distillation via Unlabeled Data</h1>
          

          
            <span class="post-meta">Posted on February 20, 2023</span>
            
            
              <!--- "ReadTime on GitHub Jekyll" (c) 2020 Ruby Griffith Ramirez, MIT License -->






  
  <span class="post-meta"><span class="d-none d-md-inline middot">&middot;</span> 1 minute read</span>


            
          
        </div>
      </div>
    </div>
  </div>
  <span class='img-desc'></span>
</div>

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Explicit and Implicit Knowledge Distillation via Unlabeled Data</h1>
          

          
            <span class="post-meta">Posted on February 20, 2023</span>
            
            
              <!--- "ReadTime on GitHub Jekyll" (c) 2020 Ruby Griffith Ramirez, MIT License -->






  
  <span class="post-meta"><span class="d-none d-md-inline middot">&middot;</span> 1 minute read</span>


            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <article role="main" class="blog-post">
        <h2 id="fire-explicit-and-implicit-knowledge-distillation-via-unlabeled-data-fire">:fire: Explicit and Implicit Knowledge Distillation via Unlabeled Data :fire:</h2>
<p><strong>Date: 2023-02-20</strong> <a href="https://arxiv.org/abs/2302.08771">:link:</a> #tag/computer_vision #tag/knowledge-distillation</p>
<h3 id="key-idea">Key idea</h3>
<p>this paper proposed a knowledge distillation method that consider prediction entropy as a method for selecting subsitute dataset from wild (a vein of knowledge distillation method that make use of unlabeld public dataset). Major components compose of: 1) a adaptive threshold selection. Use entropy as a way to select samples from a dataset. 2) class-dropping noise suppresion. Use confidence to generate pseudo labels (multi-label). 3) feature distillation component. pick two layers in NN, one from the firt BN layer which represent low level features (claimed to boost the convergence speed), the other from the last layer before final linear layer which is used to mimic the output of the teacher model. 4) last is structure differentiation relationships which constrain the difference of the output logits of teacher and student should be close.</p>

<p>:thumbsup: The design of multiple component to suppress either the label noise or imitate the structured differentiation is interesting.</p>

<p>:thumbsdown: There are no much novelty here. More or less like putting pices together to get a dish.</p>

<h3 id="takeway">Takeway</h3>

<h3 id="idea">Idea</h3>

<ul>
  <li>feature distillation part is too loose. Is there any better way to improve the convergence?</li>
  <li>the final loss term is too many. Which is the most foundimental component lead to such performance boost.</li>
  <li>what if the origional model is not robust, can student model inherite them?</li>
</ul>

<hr />
<hr />

<h2 id="fire-tamuna-accelerated-federated-learning-with-local-training-and-partial-participation-fire">:fire: TAMUNA: Accelerated Federated Learning with Local Training and Partial Participation :fire:</h2>
<p><strong>Date: 2023-02-20</strong> <a href="https://arxiv.org/abs/2302.09832">:link:</a> #tag/federated_learning</p>
<h3 id="key-idea-1">Key idea</h3>
<p>This paper proposed a novel federated learning algorithm to reduce communication complexity. The method is a “PP” version of Scaffnew.
<img src="20230220232301.png" alt="" /><br />
:thumbsup: A compreshnsive review of past work and convey the idea very clearly.</p>

<p>:thumbsdown: Simulate on a sythetic dataset. Experiment is too thin.</p>

<h3 id="takeway-1">Takeway</h3>
<ul>
  <li>several FL algorithms
    <ul>
      <li><strong>scallfold</strong> is proposed to correct fedavg under no-iid setting with linear converge speed, similar in <strong>S-Local-GD</strong> and <strong>FedLin</strong>. However, their communication complexity is $O(klog\epsilon^{-1})$</li>
      <li><strong>Scaffnew</strong> is proposed to improve the communication efficiency with improved complexity of $O(\sqrt{k}log\epsilon^{-1})$. Similarly for <strong>APDA-Inexact</strong> and <strong>5GCS</strong></li>
    </ul>
  </li>
</ul>

<h3 id="idea-1">Idea</h3>
<ul>
  <li>probabaly better to refer to Scaffnew paper to get more idea…</li>
</ul>

<hr />

      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#paper reading">paper reading</a>
          
            <a href="/tags#knowledge distillation">knowledge distillation</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/2023-02-19-self.distillation/" data-toggle="tooltip" data-placement="top" title="Robust Representation Learning with Self-Distillation for Domain Generalization">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/2023-03-11-linux-command-cheatsheet/" data-toggle="tooltip" data-placement="top" title="Linux command cheat sheet (Basic)">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  
  
  

  


  

  



    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="https://github.com/PL97" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://twitter.com/LePeng57300967" title="Twitter">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Twitter</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/le-peng-98668b204" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://scholar.google.com/citations?user=K0BZOpsAAAAJ&hl=en" title="Google Scholar">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Google Scholar</span>
    </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Le Peng
        &nbsp;&bull;&nbsp;
      
      2023

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://localhost:4003/">lepeng.org</a>
        </span>
      

      

      

      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  





  
    
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>


  



  
    <script src="/assets/js/anchor-js-config.js"></script>
  



</body>
</html>
