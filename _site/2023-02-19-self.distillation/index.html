<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Robust Representation Learning with Self-Distillation for Domain Generalization</title>

  
  <meta name="author" content="Le Peng">
  

  <meta name="description" content=":fire: Robust Representation Learning with Self-Distillation for Domain Generalization :fire: Date: 2023-02-19 :link: #tag/computer_vision #tag/self-distillation Key idea utilize self-distillation to learn domain-invariant features to improve domain generalization. It is claimed to be the first few work on ViT key design: feed origional image x to NN to get logit output...">

  

  

  
  <link rel="alternate" type="application/rss+xml" title="Le Peng" href="http://localhost:4003/feed.xml">
  

  

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Le Peng">
  <meta property="og:title" content="Robust Representation Learning with Self-Distillation for Domain Generalization">
  <meta property="og:description" content=":fire: Robust Representation Learning with Self-Distillation for Domain Generalization :fire: Date: 2023-02-19 :link: #tag/computer_vision #tag/self-distillation Key idea utilize self-distillation to learn domain-invariant features to improve domain generalization. It is claimed to be the first few work on ViT key design: feed origional image x to NN to get logit output...">

  
  <meta property="og:image" content="http://localhost:4003/assets/img/lepeng.png">
  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Le Peng">
  <meta property="og:article:published_time" content="2023-02-19T00:00:00-05:00">
  <meta property="og:url" content="http://localhost:4003/2023-02-19-self.distillation/">
  <link rel="canonical" href="http://localhost:4003/2023-02-19-self.distillation/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@LePeng57300967">
  <meta name="twitter:creator" content="@LePeng57300967">

  <meta property="twitter:title" content="Robust Representation Learning with Self-Distillation for Domain Generalization">
  <meta property="twitter:description" content=":fire: Robust Representation Learning with Self-Distillation for Domain Generalization :fire: Date: 2023-02-19 :link: #tag/computer_vision #tag/self-distillation Key idea utilize self-distillation to learn domain-invariant features to improve domain generalization. It is claimed to be the first few work on ViT key design: feed origional image x to NN to get logit output...">

  
  <meta name="twitter:image" content="http://localhost:4003/assets/img/lepeng.png">
  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4003/">Le Peng</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/pages/publication">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/pages/services">Service</a>
          </li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Notes</a>
            <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/pages/blog">Posts</a>
                  <a class="dropdown-item" href="/pages/tags">Tags</a>
            </div>
          </li>
        
          <li class="nav-item">
            <a class="nav-link" href="/index">About Me</a>
          </li></ul>
  </div>

  

  

</nav>





  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Robust Representation Learning with Self-Distillation for Domain Generalization</h1>
          

          
            <span class="post-meta">Posted on February 19, 2023</span>
            
            
              <!--- "ReadTime on GitHub Jekyll" (c) 2020 Ruby Griffith Ramirez, MIT License -->






  
  <span class="post-meta"><span class="d-none d-md-inline middot">&middot;</span> 2 minute read</span>


            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <article role="main" class="blog-post">
        <h2 id="fire-robust-representation-learning-with-self-distillation-for-domain-generalization-fire">:fire: Robust Representation Learning with Self-Distillation for Domain Generalization :fire:</h2>
<p><strong>Date: 2023-02-19</strong> <a href="https://arxiv.org/abs/2302.06874">:link:</a>
#tag/computer_vision #tag/self-distillation</p>
<h3 id="key-idea">Key idea</h3>
<p>utilize self-distillation to learn domain-invariant features to improve domain generalization. It is claimed to be the first few work on ViT</p>

<h4 id="key-design">key design:</h4>
<p><img src="20230219151050.png" alt="" /></p>

<ul>
  <li>feed origional image x to NN to get logit output $p(f(x’))$</li>
  <li>apply data augmentation to image x to get image x’</li>
  <li>calculate all intermiediate feature output and feed them to a fc layer f’ to get logit $p(f’(x’))$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>calculate the KL divergence $L(p(f(x))</td>
          <td> </td>
          <td>p(f’(x))) as L_i$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>calculate the KL divergence $L(p(f(x))</td>
          <td> </td>
          <td>p(f’(x’))) as L_a$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>the final loss is a compose of $L = L_ce + \lambda L_i + \gamma L_a$
    <h3 id="takeway">Takeway</h3>
  </li>
  <li>self-distillation can be combined with data augmentation to achieve better robustness</li>
  <li>tools: AutoAugment: <a href="https://arxiv.org/pdf/1805.09501.pdf">:books:</a> <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html">:hammer:</a></li>
</ul>

<h3 id="idea">Idea</h3>
<ul>
  <li>Is there any more quantitive way of measure domain generalization and directly optimize it?</li>
  <li>For domain adaption where we have access to the target domain data, how can we achive better performance?
    <ul>
      <li>a possible solution would be that we can disentangle the domain invariant and sepecific features. For example, we can adopt the two-encoder structure to learn features separately and try to maximize the correlation of invariant features (probbaly more accurate if model the conditional probability) and minimize the correlation of invariant and specific features.</li>
    </ul>
  </li>
</ul>

<hr />
<hr />

<h2 id="fire-revisiting-self-distillation-fire">:fire: Revisiting Self-Distillation :fire:</h2>
<p><strong>Date: 2023-02-19</strong> <a href="https://arxiv.org/abs/2206.08491">:link:</a>
#tag/computer_vision #tag/self-distillation</p>
<h3 id="key-idea-1">Key idea</h3>
<p>This paper shows that 1) in self-distillation(both teacher and student model share the same model archicture) student model can always surpass teacher model 2) existing theoretical explation of self-distillation is problematic 3) the success of self-distillation can be explained by the loss landscape geometry</p>

<p>:thumbsup: the analysis is multi-facet: they first compare ensemble</p>

<p>:thumbsdown: Their evidence of disprove multi-view theory is not very convincing. When the author proves that multi-view theory cannot explain that when increasing the self-distillation round, the student’s model performance does not increase accordingly. This can be possible that the model lack of capacity to learn all the features. If same finding can be shown on large model, it would be more sound. There are also many results that have little discussion, for example, why self-distillation outperform SAM model. Discussion on Figure 8 about the round 2 loss surface coutour.</p>

<h3 id="takeway-1">Takeway</h3>
<ul>
  <li>ensemble model perform better than Born-again Neural Networks(BAN). When performing multi-round self-distillation, the gain graduately vanished.</li>
  <li>PyHessian <a href="https://github.com/amirgholami/PyHessian">:hammer:</a></li>
  <li>Sharpness-Aware Minimization (SAM) 
<a href="https://arxiv.org/pdf/2010.01412.pdf">:books:</a> 
<a href="https://github.com/google-research/sam">:hammer:</a></li>
  <li>visualize loss contour <a href="https://mathformachines.com/posts/visualizing-the-loss-landscape/">:hammer:</a></li>
</ul>

<h3 id="idea-1">Idea</h3>
<ul>
  <li>Some evidence shows that ensemble model always surpass self-distill model (although there is always gains from making the model more compact). Any support for that?</li>
  <li>Since performing multi-round distillation the gain can vanish. Is there any good way of doing early stopping?</li>
</ul>


      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#paper reading">paper reading</a>
          
            <a href="/tags#self distillation">self distillation</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        
        <li class="page-item next">
          <a class="page-link" href="/2023-02-20-knowledge-distillation/" data-toggle="tooltip" data-placement="top" title="Explicit and Implicit Knowledge Distillation via Unlabeled Data">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  
  
  

  


  

  



    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="https://github.com/PL97" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://twitter.com/LePeng57300967" title="Twitter">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Twitter</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/le-peng-98668b204" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://scholar.google.com/citations?user=K0BZOpsAAAAJ&hl=en" title="Google Scholar">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Google Scholar</span>
    </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Le Peng
        &nbsp;&bull;&nbsp;
      
      2023

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="http://localhost:4003/">lepeng.org</a>
        </span>
      

      

      

      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  





  
    
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>


  



  
    <script src="/assets/js/anchor-js-config.js"></script>
  



</body>
</html>
